# Agentic-JIT
MUST READ: @AGENTS.md, @.agents/agentic-jit-framework.md
CRITICAL, ONLY read when the provided task is making ANY updates to code: .agents/context-refresh-playbook.md


# Init
Project context: @_bmad-output/planning-artifacts/product-brief.md, @_bmad-output/planning-artifacts/project-context.md, _bmad-output/planning-artifacts/prd/index.md, _bmad-output/planning-artifacts/architecture/index.md, _bmad-output/planning-artifacts/epics/index.md

Implemented stories: _bmad-output/implementation-artifacts/
Recent update Changelogs: docs/changelog/

- Append all updates that should be referenced by QA according to CHANGELOG.md. 
- Unrecognized changes: assume other agent; keep going; focus your changes. If it causes issues, stop + ask user. Pass this to subagents as well
- YOU are the implementor. Read extra files yourself iff necessary, BE HYPER context-efficient and offload any research / search / verification / exploration task to sub-agents by default. ONLY use the Context7 MCP through subagents You are the consumer and implementor, not the researcher.
- Offload simple / manual tasks to subagents and scripts. ie) offload 'manual' tasks and anything that doesn't require active thinking in the process. i18n is a great example.
- Based on the difficulty of each task, assign opus / sonnet / haiku adaptively for subagents. Use haiku ONLY for VERY SIMPLE manual tasks, for any case that touches backend code or important logic, use opus by default.


# Prompt
CRITICAL: Execute each step sequentially.

[TASKS]
STEP1: Use multiple subagents to scan the docs on what was implemented to understand user journeys and features that should be e2e tested.

STEP2: Based on the research, go through the project actual implementation using multiple subagents to get a plan on how to create the e2e tests. Note that they already exist so make sure there are no duplicates.

STEP3: Consolidate the information and use multiple subagents to create the e2e tests

STEP4: Use multiple subagents to verify the e2e tests with implementation expectations. Keep in mind if the implementation is off the tests SHOULD fail.

STEP5: Run the e2e tests, check the results and use multiple subagents to address ALL issues in parallel

STEP6: Repeat STEP5 until no issues

STEP7: Update CHANGELOG and provide a summary of the entire run.



Create a worktree, enter worktree, and implement 